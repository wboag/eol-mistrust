{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Outcomes Using Trust Features</h1>\n",
    "Does performance improve for tasks (code status, leaving AMA, and in-hosp mortality) when adding mistrust features on top of demographics? \n",
    "Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "import tqdm\n",
    "\n",
    "con = psycopg2.connect(dbname ='mimic', user='wboag', host=\"/var/run/postgresql\")\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-12 20:31:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46121it [00:06, 7397.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>label</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100003</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Full code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100006</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Full Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100007</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Full Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100009</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Full code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100011</td>\n",
       "      <td>Code Status</td>\n",
       "      <td>Full code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id        label      value\n",
       "0   100003  Code Status  Full code\n",
       "1   100006  Code Status  Full Code\n",
       "2   100007  Code Status  Full Code\n",
       "3   100009  Code Status  Full code\n",
       "4   100011  Code Status  Full code"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "\n",
    "# LABEL: code status\n",
    "\n",
    "code_query = \"select distinct hadm_id,label,value from mimiciii.chartevents c JOIN mimiciii.d_items i on i.itemid=c.itemid where label = 'Code Status'\"\n",
    "code_status = pd.read_sql_query(code_query, con)\n",
    "\n",
    "# binary labels\n",
    "code_labels = {}\n",
    "for i,row in tqdm.tqdm(code_status.iterrows()):\n",
    "    if row.value is not None:\n",
    "        if ('DNR' in row.value) or ('DNI' in row.value) or ('Comfort' in row.value) or ('Do Not' in row.value):\n",
    "            label = 'DNR/CMO'\n",
    "        elif (row.value == 'Full Code') or (row.value == 'Full code'):\n",
    "            label = 'Full Code'\n",
    "    code_labels[row.hadm_id] = label\n",
    "    \n",
    "code_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['DNR / DNI', 'DNR (do not resuscitate)', 'Other/Remarks', 'DNI (do not intubate)', 'Comfort measures only', 'Do Not Intubate', None, 'Full code', 'Full Code', 'Comfort Measures', 'CPR Not Indicate', 'Do Not Resuscita'])\n"
     ]
    }
   ],
   "source": [
    "print set(code_status['value'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hadm -> race\n",
    "import tqdm\n",
    "\n",
    "def normalize_race(race):\n",
    "    if 'HISPANIC' in race:\n",
    "        return 'Hispanic'\n",
    "    if 'SOUTH AMERICAN' in race:\n",
    "        return 'Hispanic'\n",
    "    if 'AMERICAN INDIAN' in race:\n",
    "        return 'Native American'\n",
    "    if 'ASIAN' in race:\n",
    "        return 'Asian'\n",
    "    if 'BLACK' in race:\n",
    "        return 'Black'\n",
    "    if 'WHITE' in race:\n",
    "        return 'White'\n",
    "    return 'Other'\n",
    "\n",
    "def normalize_insurance(insurance):\n",
    "    if insurance in ['Medicare', 'Medicaid', 'Government']:\n",
    "        return 'Public'\n",
    "    else:\n",
    "        return insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58976it [00:05, 10909.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>discharge_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191772</td>\n",
       "      <td>DEAD/EXPIRED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174565</td>\n",
       "      <td>HOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177287</td>\n",
       "      <td>HOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110313</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>127542</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id discharge_location\n",
       "0   191772       DEAD/EXPIRED\n",
       "1   174565               HOME\n",
       "2   177287               HOME\n",
       "3   110313   HOME HEALTH CARE\n",
       "4   127542   HOME HEALTH CARE"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LABEL: left hospital against medical advice\n",
    "\n",
    "# query for discharge info\n",
    "discharge_query = 'SELECT distinct hadm_id,discharge_location FROM mimiciii.admissions'\n",
    "discharge = pd.read_sql_query(discharge_query, con)\n",
    "\n",
    "# binary labels\n",
    "ama_labels = {}\n",
    "for i,row in tqdm.tqdm(discharge.iterrows()):\n",
    "    if row.discharge_location == 'LEFT AGAINST MEDICAL ADVI':\n",
    "        label = 'AMA'\n",
    "    else:\n",
    "        label = 'compliant'\n",
    "    ama_labels[row.hadm_id] = label\n",
    "\n",
    "discharge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58976it [00:04, 12035.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111668</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194641</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id  hospital_expire_flag\n",
       "0   150909                     0\n",
       "1   111668                     0\n",
       "2   194641                     0\n",
       "3   193891                     0\n",
       "4   166563                     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LABEL: in-hospital mortality\n",
    "\n",
    "# query for discharge info\n",
    "mortality_query = 'SELECT distinct hadm_id,hospital_expire_flag FROM mimiciii.admissions'\n",
    "mortality = pd.read_sql_query(mortality_query, con)\n",
    "\n",
    "# binary labels\n",
    "mortality_labels = {}\n",
    "for i,row in tqdm.tqdm(mortality.iterrows()):\n",
    "    if row.hospital_expire_flag:\n",
    "        label = 'deceased'\n",
    "    else:\n",
    "        label = 'survived'\n",
    "    mortality_labels[row.hadm_id] = label\n",
    "\n",
    "mortality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_split(ids, ratio=0.6):\n",
    "    random.shuffle(ids)\n",
    "    train = ids[:int(len(ids)*ratio) ]\n",
    "    test  = ids[ int(len(ids)*ratio):]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write informative features code\n",
    "\n",
    "def analyze(task, vect, clf, count_top=False):\n",
    "\n",
    "    ind2feat =  { i:f for f,i in vect.vocabulary_.items() }\n",
    "\n",
    "    # create a 2-by-m matrix for biary, rather than relying on 1-p bullshit\n",
    "    coef_ = clf.coef_\n",
    "    \n",
    "    # most informative features\n",
    "    #\"\"\"\n",
    "    print task\n",
    "    informative_feats = np.argsort(coef_)\n",
    "    \n",
    "    if len(informative_feats.shape) == 2:\n",
    "        informative_feats = informative_feats[0,:]\n",
    "        coef_ = coef_[0,:]\n",
    "        \n",
    "    #'''\n",
    "    # display what each feature is\n",
    "    for feat in reversed(informative_feats):\n",
    "        val = coef_[feat]\n",
    "\n",
    "        word = ind2feat[feat]\n",
    "        print '\\t%-25s: %7.4f' % (word,val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import pylab as plt\n",
    "\n",
    "\n",
    "def compute_stats(task, pred, P, ref, labels_map, verbose):\n",
    "    if len(labels_map) == 2:\n",
    "        scores = P[:,1] - P[:,0]\n",
    "        res = compute_stats_binary(    task, pred, scores, ref, labels_map, verbose)\n",
    "    else:\n",
    "        res = compute_stats_multiclass(task, pred, P     , ref, labels_map, verbose)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def compute_stats_binary(task, pred, P, ref, labels, verbose):\n",
    "    # santiy check\n",
    "    assert all(map(int,P>0) == pred)\n",
    "\n",
    "    V = [0,1]\n",
    "    n = len(V)\n",
    "    assert n==2, 'sorry, must be exactly two labels (how else would we do AUC?)'\n",
    "    conf = np.zeros((n,n), dtype='int32')\n",
    "    for p,r in zip(pred,ref):\n",
    "        conf[p][r] += 1\n",
    "\n",
    "    if verbose:\n",
    "        print conf\n",
    "        print\n",
    "    \n",
    "    tp = conf[1,1]\n",
    "    tn = conf[0,0]\n",
    "    fp = conf[1,0]\n",
    "    fn = conf[0,1]\n",
    "\n",
    "    precision   = tp / (tp + fp + 1e-9)\n",
    "    recall      = tp / (tp + fn + 1e-9)\n",
    "    sensitivity = tp / (tp + fn + 1e-9)\n",
    "    specificity = tn / (tn + fp + 1e-9)\n",
    "\n",
    "    f1 = (2*precision*recall) / (precision+recall+1e-9)\n",
    "\n",
    "    tpr =  true_positive_rate(pred, ref)\n",
    "    fpr = false_positive_rate(pred, ref)\n",
    "\n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn + 1e-9)\n",
    "    \n",
    "    if verbose:\n",
    "        print '\\tspecificity %.3f' % specificity\n",
    "        print '\\tsensitivty: %.3f' % sensitivity\n",
    "\n",
    "    # AUC\n",
    "    if len(set(ref)) == 2:\n",
    "        auc = sklearn.metrics.roc_auc_score(ref, P)\n",
    "        if verbose: print '\\t\\tauc:        %.3f' % auc\n",
    "\n",
    "    if verbose:\n",
    "        print '\\taccuracy:   %.3f' % accuracy\n",
    "        print '\\tprecision:  %.3f' % precision\n",
    "        print '\\trecall:     %.3f' % recall\n",
    "        print '\\tf1:         %.3f' % f1\n",
    "        print '\\tTPR:        %.3f' % tpr\n",
    "        print '\\tFPR:        %.3f' % fpr\n",
    "\n",
    "        print 'TODO: VIZ THE ROC CURVE'\n",
    "\n",
    "    res = {'accuracy':accuracy, 'precision':precision, 'recall':recall, 'f1':f1, 'tpr':tpr,\n",
    "           'fpr':fpr, 'auc':auc, 'sensitivity':sensitivity, 'specificity':specificity}\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def compute_stats_multiclass(task, pred, P, ref, labels_map):\n",
    "    # santiy check\n",
    "    assert all(map(int,P.argmax(axis=1)) == pred)\n",
    "\n",
    "    # get rid of that final prediction dimension\n",
    "    #pred = pred[1:]\n",
    "    #ref  =  ref[1:]\n",
    "\n",
    "    V = set(range(len(labels_map)))\n",
    "    n = max(V)+1\n",
    "    conf = np.zeros((n,n), dtype='int32')\n",
    "    for p,r in zip(pred,ref):\n",
    "        conf[p][r] += 1\n",
    "\n",
    "\n",
    "    labels = [label for label,i in sorted(labels_map.items(), key=lambda t:t[1])]\n",
    "\n",
    "\n",
    "    print conf\n",
    "    print\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    print '\\t prec  rec    f1   label'\n",
    "    for i in range(n):\n",
    "        label = labels[i]\n",
    "\n",
    "        tp = conf[i,i]\n",
    "        pred_pos = conf[i,:].sum()\n",
    "        ref_pos  = conf[:,i].sum()\n",
    "\n",
    "        precision   = tp / (pred_pos + 1e-9)\n",
    "        recall      = tp / (ref_pos + 1e-9)\n",
    "        f1 = (2*precision*recall) / (precision+recall+1e-9)\n",
    "\n",
    "        print '\\t%.3f %.3f %.3f %s' % (precision,recall,f1,label)\n",
    "\n",
    "        # Save info\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall    = sum(recalls   ) / len(recalls   )\n",
    "    avg_f1        = sum(f1s       ) / len(f1s       )\n",
    "    print '\\t--------------------------'\n",
    "    print '\\t%.3f %.3f %.3f avg' % (avg_precision,avg_recall,avg_f1)\n",
    "\n",
    "    print 'TODO: VIZ THE F1S'\n",
    "\n",
    "    \n",
    "    res = {'precisions':precisions, 'recalls':recalls, 'f1s':f1s}\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def true_positive_rate(pred, ref):\n",
    "    tp,fn = 0,0\n",
    "    for p,r in zip(pred,ref):\n",
    "        if p==1 and r==1:\n",
    "            tp += 1\n",
    "        elif p==0 and r==1:\n",
    "            fn += 1\n",
    "    return tp / (tp + fn + 1e-9)\n",
    "\n",
    "\n",
    "def false_positive_rate(pred, ref):\n",
    "    fp,tn = 0,0\n",
    "    for p,r in zip(pred,ref):\n",
    "        if p==1 and r==0:\n",
    "            fp += 1\n",
    "        elif p==0 and r==0:\n",
    "            tn += 1\n",
    "    return fp / (fp + tn + 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classification_results(svm, labels_map, X, Y, task, verbose=True):\n",
    "\n",
    "    # for AUC\n",
    "    P_ = svm.decision_function(X)\n",
    "\n",
    "    # sklearn has stupid-ass changes in API when doing binary classification. make it conform to 3+\n",
    "    if len(labels_map)==2:\n",
    "        m = X.shape[0]\n",
    "        P = np.zeros((m,2))\n",
    "        P[:,0] = -P_\n",
    "        P[:,1] =  P_\n",
    "    else:\n",
    "        P = P_\n",
    "\n",
    "    train_pred = P.argmax(axis=1)\n",
    "\n",
    "    # what is the predicted vocab without the dummy label?\n",
    "    V = labels_map.keys()\n",
    "\n",
    "    if verbose: print task\n",
    "    res = compute_stats(task, train_pred, P, Y, labels_map, verbose)\n",
    "    if verbose: print '\\n'\n",
    "    return res\n",
    "    \n",
    "\n",
    "\n",
    "def regression_results(lr, test_X, test_Y, description, verbose=True):\n",
    "    res = {}\n",
    "    \n",
    "    pred_Y = lr.predict(test_X)\n",
    "    res['rms'] = sqrt(mean_squared_error(test_Y, pred_Y))\n",
    "    res['mas'] = mean_absolute_error(test_Y, pred_Y)\n",
    "    if verbose:\n",
    "        print description\n",
    "        print '\\tRMS:', res['rms']\n",
    "        print '\\tMAS:', res['mas']\n",
    "        print\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        perfect = np.arange(min(test_Y),max(test_Y),100)\n",
    "        plt.scatter(perfect, perfect, color='red', s=0.01)\n",
    "        plt.scatter(test_Y , pred_Y, color='blue', s=1)\n",
    "        plt.xlabel('actual')\n",
    "        plt.ylabel('prediction')\n",
    "        plt.show()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noncompliant: 54510\n",
      "autopsy: 54510\n",
      "sentiment: 48273\n",
      "extra1: 61382\n",
      "extra2: 58009\n",
      "extra3: 58009\n",
      "extra4: 51522\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>insurance</th>\n",
       "      <th>oasis</th>\n",
       "      <th>noncompliant</th>\n",
       "      <th>autopsy</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191826</td>\n",
       "      <td>Public</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.659708</td>\n",
       "      <td>-0.430902</td>\n",
       "      <td>0.267377</td>\n",
       "      <td>M</td>\n",
       "      <td>80.6794</td>\n",
       "      <td>White</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>10.2431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127133</td>\n",
       "      <td>Private</td>\n",
       "      <td>29</td>\n",
       "      <td>1.696974</td>\n",
       "      <td>-1.410404</td>\n",
       "      <td>-0.336874</td>\n",
       "      <td>M</td>\n",
       "      <td>63.4036</td>\n",
       "      <td>White</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>29.9319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127133</td>\n",
       "      <td>Private</td>\n",
       "      <td>47</td>\n",
       "      <td>1.696974</td>\n",
       "      <td>-1.410404</td>\n",
       "      <td>-0.336874</td>\n",
       "      <td>M</td>\n",
       "      <td>63.4036</td>\n",
       "      <td>White</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>29.9319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110408</td>\n",
       "      <td>Public</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.806254</td>\n",
       "      <td>-0.424124</td>\n",
       "      <td>-0.450595</td>\n",
       "      <td>F</td>\n",
       "      <td>64.0647</td>\n",
       "      <td>Other</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>15.6868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191517</td>\n",
       "      <td>Private</td>\n",
       "      <td>23</td>\n",
       "      <td>0.235403</td>\n",
       "      <td>0.555377</td>\n",
       "      <td>1.184329</td>\n",
       "      <td>M</td>\n",
       "      <td>51.8053</td>\n",
       "      <td>White</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>3.1125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id insurance  oasis  noncompliant   autopsy  sentiment gender  \\\n",
       "0   191826    Public     22     -0.659708 -0.430902   0.267377      M   \n",
       "1   127133   Private     29      1.696974 -1.410404  -0.336874      M   \n",
       "2   127133   Private     47      1.696974 -1.410404  -0.336874      M   \n",
       "3   110408    Public     32     -0.806254 -0.424124  -0.450595      F   \n",
       "4   191517   Private     23      0.235403  0.555377   1.184329      M   \n",
       "\n",
       "       age   race admission_type      los  \n",
       "0  80.6794  White       ELECTIVE  10.2431  \n",
       "1  63.4036  White      EMERGENCY  29.9319  \n",
       "2  63.4036  White      EMERGENCY  29.9319  \n",
       "3  64.0647  Other      EMERGENCY  15.6868  \n",
       "4  51.8053  White      EMERGENCY   3.1125  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load features\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "def normalize(scores):\n",
    "    vals = np.array(scores.values())\n",
    "    mu = vals.mean()\n",
    "    std = vals.std()\n",
    "    return { k:(v-mu)/std for k,v in scores.items()}\n",
    "\n",
    "\n",
    "# query for insurance info\n",
    "insurance_query = 'SELECT distinct hadm_id,insurance FROM mimiciii.admissions'\n",
    "insurance = pd.read_sql_query(insurance_query, con)\n",
    "\n",
    "# query for oasis info\n",
    "oasis_query = 'SELECT distinct hadm_id,oasis FROM mimiciii.oasis'\n",
    "oasis = pd.read_sql_query(oasis_query, con)\n",
    "\n",
    "# query for demographics info\n",
    "patients_query = 'SELECT distinct hadm_id,gender,age,ethnicity,admission_type,los_hospital FROM mimiciii.icustay_detail'\n",
    "patients = pd.read_sql_query(patients_query, con)\n",
    "patients = patients.loc[patients['admission_type']!='NEWBORN']\n",
    "\n",
    "# Load trust scores\n",
    "with open('../data/mistrust_noncompliant.pkl', 'rb') as f:\n",
    "    noncompliant_dict = normalize(pickle.load(f))\n",
    "print 'noncompliant:', len(noncompliant_dict)\n",
    "noncompliant_df = pd.DataFrame(noncompliant_dict.items(), columns=['hadm_id','noncompliant'])\n",
    "\n",
    "# Load trust scores\n",
    "with open('../data/mistrust_autopsy.pkl', 'rb') as f:\n",
    "    autopsy_dict = normalize(pickle.load(f))\n",
    "print 'autopsy:', len(autopsy_dict)\n",
    "autopsy_df = pd.DataFrame(autopsy_dict.items(), columns=['hadm_id','autopsy'])\n",
    "\n",
    "# Load trust scores\n",
    "with open('../data/neg_sentiment.pkl', 'rb') as f:\n",
    "    sentiment_dict = normalize(pickle.load(f))\n",
    "print 'sentiment:', len(sentiment_dict)\n",
    "sentiment_df = pd.DataFrame(sentiment_dict.items(), columns=['hadm_id','sentiment'])\n",
    "\n",
    "    \n",
    "# merge data\n",
    "extra_1 = pd.merge(insurance, oasis, on=['hadm_id'])\n",
    "extra_2 = pd.merge(extra_1, noncompliant_df, on=['hadm_id'])\n",
    "extra_3 = pd.merge(extra_2, autopsy_df     , on=['hadm_id'])\n",
    "extra_4 = pd.merge(extra_3, sentiment_df   , on=['hadm_id'])\n",
    "demographics = pd.merge(extra_4, patients  , on=['hadm_id'])\n",
    "\n",
    "# Normalize some columns\n",
    "demographics['ethnicity'] = demographics['ethnicity'].apply(normalize_race)\n",
    "demographics['insurance'] = demographics['insurance'].apply(normalize_insurance)\n",
    "demographics = demographics.rename(columns={'ethnicity':'race'})\n",
    "demographics = demographics.rename(columns={'los_hospital':'los'})\n",
    "\n",
    "demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-12 16:58:26\n",
      "2018-05-12 16:58:26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "print strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "def normalize_mean_std(value, mu, std):\n",
    "    return (value-mu)/std\n",
    "    \n",
    "# normalize ages\n",
    "ages = np.array(demographics['age'])\n",
    "age_mu = ages.mean()\n",
    "age_std = ages.std()\n",
    "demographics['age'] = demographics['age'].apply(lambda val:normalize_mean_std(val,age_mu,age_std))\n",
    "\n",
    "# normalize oasis scores\n",
    "oasis = np.array(demographics['oasis'])\n",
    "oasis_mu = oasis.mean()\n",
    "oasis_std = oasis.std()\n",
    "demographics['oasis'] = demographics['oasis'].apply(lambda val:normalize_mean_std(val,oasis_mu,oasis_std))\n",
    "\n",
    "# normalize los scores\n",
    "los = np.array(demographics['los'])\n",
    "los_mu = los.mean()\n",
    "los_std = los.std()\n",
    "demographics['los'] = demographics['los'].apply(lambda val:normalize_mean_std(val,los_mu,los_std))\n",
    "\n",
    "# foo\n",
    "\n",
    "def build_features(enabled):\n",
    "    demographics_features = {}\n",
    "    for i,row in tqdm.tqdm(demographics.iterrows()):\n",
    "        feats = {}\n",
    "\n",
    "        if 'admission_type' in enabled: feats[('admission_type', row.admission_type   )] = 1\n",
    "        if 'oasis'          in enabled: feats[('oasis', None)] = row.oasis\n",
    "\n",
    "        if 'age' in enabled: feats[('age'  , None)] = row.age\n",
    "        if 'los' in enabled: feats[('los'  , None)] = row.los\n",
    "\n",
    "        if 'insurance' in enabled: feats[('insurance'     , row.insurance)] = 1\n",
    "        if 'gender'    in enabled: feats[('gender'        , row.gender   )] = 1\n",
    "\n",
    "        if 'race'     in enabled: feats[('race', row.race     )] = 1\n",
    "            \n",
    "        if 'noncompliant' in enabled: feats[('concompliant',None)] = row.noncompliant\n",
    "        if 'autopsy'      in enabled: feats[('autopsy'     ,None)] = row.autopsy\n",
    "        if 'sentiment'    in enabled: feats[('sentiment'   ,None)] = row.sentiment\n",
    "\n",
    "        demographics_features[row.hadm_id] = feats\n",
    "\n",
    "    print strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # fit vectorizer\n",
    "    vect = DictVectorizer()\n",
    "    vect.fit(demographics_features.values())\n",
    "    print 'num_features:', len(vect.get_feature_names())\n",
    "\n",
    "    # ordering of all features\n",
    "    ids = demographics_features.keys()\n",
    "    print '\\t', strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    X = vect.transform([demographics_features[hadm_id] for hadm_id in ids])    \n",
    "\n",
    "    return demographics_features, vect\n",
    "    \n",
    "print strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 199.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-12 18:13:31\n",
      "BASELINE+ALL\n",
      "['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51522it [00:08, 6189.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-12 18:13:40\n",
      "num_features: 16\n",
      "\t2018-05-12 18:13:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients: 48071\n",
      "Counter({0: 47746, 1: 325})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:01<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCS:  [0.86304508 0.8898861  0.88454724 0.87209732 0.8691665  0.88331788\n",
      " 0.89105331 0.84446035 0.87904439 0.84483964 0.86385253 0.87041656\n",
      " 0.88775113 0.8785297  0.89844164 0.88767295 0.87012795 0.87562579\n",
      " 0.88172111 0.87176786 0.87337841 0.86876047 0.87283639 0.83836708\n",
      " 0.86955035 0.86759934 0.87989298 0.89410665 0.85591779 0.86279355\n",
      " 0.88124919 0.89341509 0.87808205 0.86907466 0.87280057 0.85670457\n",
      " 0.87209048 0.86640638 0.86984129 0.89132035 0.8632953  0.89894989\n",
      " 0.87027522 0.87446934 0.89029219 0.86157223 0.87621635 0.88085429\n",
      " 0.89322966 0.87153019 0.87043719 0.87260386 0.86098986 0.8868004\n",
      " 0.87057264 0.88764782 0.8840213  0.86125103 0.8625529  0.87929983\n",
      " 0.8500925  0.86213968 0.87824507 0.88079966 0.88007555 0.88075165\n",
      " 0.86605238 0.87218972 0.90277366 0.87567829 0.87541977 0.88838886\n",
      " 0.83465103 0.87513934 0.83261367 0.88089711 0.83799851 0.88093045\n",
      " 0.85280004 0.86700751 0.86270631 0.87246667 0.86793567 0.86684598\n",
      " 0.8430358  0.86769759 0.85548898 0.8420679  0.88252209 0.86068304\n",
      " 0.87574668 0.89551483 0.88001663 0.87909713 0.85267144 0.87984087\n",
      " 0.89932503 0.86016361 0.87191644 0.86481711]\n",
      "    mean:      0.872136184306181\n",
      "    1.96*std:  0.02862789071415167\n",
      "    conf_interval: (0.8435,0.9008)\n",
      "ama\n",
      "\t('concompliant', None)   :  0.5506\n",
      "\t('sentiment', None)      :  0.0000\n",
      "\t('race', 'White')        :  0.0000\n",
      "\t('race', 'Native American'):  0.0000\n",
      "\t('race', 'Hispanic')     :  0.0000\n",
      "\t('race', 'Black')        :  0.0000\n",
      "\t('race', 'Asian')        :  0.0000\n",
      "\t('insurance', 'Self Pay'):  0.0000\n",
      "\t('insurance', 'Public')  :  0.0000\n",
      "\t('gender', 'M')          :  0.0000\n",
      "\t('autopsy', None)        :  0.0000\n",
      "\t('race', 'Other')        : -0.1891\n",
      "\t('gender', 'F')          : -0.6396\n",
      "\t('insurance', 'Private') : -1.1397\n",
      "\t('los', None)            : -1.4199\n",
      "\t('age', None)            : -2.0333\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "age       :None            || -2.10 +/- 0.21\n",
      "autopsy   :None            || 0.01 +/- 0.03\n",
      "concompliant:None            || 0.52 +/- 0.09\n",
      "gender    :F               || -0.40 +/- 0.20\n",
      "gender    :M               || 0.00 +/- 0.00\n",
      "insurance :Private         || -1.01 +/- 0.21\n",
      "insurance :Public          || 0.00 +/- 0.00\n",
      "insurance :Self Pay        || 0.00 +/- 0.00\n",
      "los       :None            || -1.44 +/- 0.37\n",
      "race      :Asian           || 0.00 +/- 0.00\n",
      "race      :Black           || 0.03 +/- 0.12\n",
      "race      :Hispanic        || 0.00 +/- 0.00\n",
      "race      :Native American || 0.00 +/- 0.00\n",
      "race      :Other           || -0.15 +/- 0.19\n",
      "race      :White           || -0.02 +/- 0.06\n",
      "sentiment :None            || -0.00 +/- 0.02\n",
      "2018-05-12 18:14:42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AMA\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "print strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "featlists = {\n",
    "                #'BASELINE'             :['age', 'los', 'insurance', 'gender'],\n",
    "                #'BASELINE+RACE'        :['age', 'los', 'insurance', 'gender', 'race'],\n",
    "                #'BASELINE+NONCOMPLIANT':['age', 'los', 'insurance', 'gender', 'noncompliant'],\n",
    "                #'BASELINE+AUTOPSY'     :['age', 'los', 'insurance', 'gender', 'autopsy'],\n",
    "                #'BASELINE+SENTIMENT'   :['age', 'los', 'insurance', 'gender', 'sentiment'],\n",
    "                'BASELINE+ALL'         :['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
    "            }\n",
    "\n",
    "ama_Y_vect = {'AMA': 1, 'compliant': 0}\n",
    "\n",
    "feature_weights = defaultdict(list)\n",
    "\n",
    "for name,featlist in featlists.items():\n",
    "    print name\n",
    "    print featlist\n",
    "    \n",
    "    demographics_features, vect = build_features(featlist)\n",
    "    ind2feat =  { i:f for f,i in vect.vocabulary_.items() }\n",
    "\n",
    "    ama_ids = list(set(discharge['hadm_id'].values) & set(demographics_features.keys()))\n",
    "    print 'patients:', len(ama_ids)\n",
    "  \n",
    "    print Counter([ama_Y_vect[ama_labels[hadm_id]] for hadm_id in ama_ids])\n",
    "\n",
    "    aucs = []\n",
    "    for iteration in tqdm.tqdm(range(100)):\n",
    "\n",
    "        # train/test split\n",
    "        ama_train_ids, ama_test_ids = data_split(ama_ids)\n",
    "\n",
    "        # select pre-computed features\n",
    "        ama_train_features = [demographics_features[hadm_id] for hadm_id in ama_train_ids]\n",
    "        ama_test_features  = [demographics_features[hadm_id] for hadm_id in ama_test_ids ]\n",
    "\n",
    "        # vectorize features\n",
    "        ama_train_X = vect.transform(ama_train_features)\n",
    "        ama_test_X  = vect.transform(ama_test_features)\n",
    "\n",
    "        # vectorize task-specific labels\n",
    "        #print ama_Y_vect\n",
    "\n",
    "        # select labels\n",
    "        ama_train_Y = [ama_Y_vect[ama_labels[hadm_id]] for hadm_id in ama_train_ids]\n",
    "        ama_test_Y  = [ama_Y_vect[ama_labels[hadm_id]] for hadm_id in ama_test_ids ]\n",
    "\n",
    "        # fit model\n",
    "        ama_svm = LogisticRegression(C=0.1, penalty='l1', tol=0.01)\n",
    "        ama_svm.fit(ama_train_X,ama_train_Y)\n",
    "        #print ama_svm\n",
    "\n",
    "\n",
    "        # AMA Model eval\n",
    "\n",
    "        # evaluate model\n",
    "        res = classification_results(ama_svm, ama_Y_vect,  ama_test_X,  ama_test_Y, 'test:  ama', verbose=False)\n",
    "        aucs.append(res['auc'])\n",
    "\n",
    "        # record the weights of the features (because we average them)\n",
    "        if name == 'BASELINE+ALL':\n",
    "            for feat,val in enumerate(ama_svm.coef_.tolist()[0]):\n",
    "                featname = ind2feat[feat]\n",
    "                feature_weights[featname].append(val)\n",
    "\n",
    "        #classification_results(ama_svm, ama_Y_vect, ama_train_X, ama_train_Y, 'train: ama')\n",
    "\n",
    "    aucs = np.array(aucs)\n",
    "    print 'AUCS: ', aucs\n",
    "    print '    mean:     ', aucs.mean()\n",
    "    print '    1.96*std: ', aucs.std() * 1.96\n",
    "    print '    conf_interval: (%.4f,%.4f)' % (aucs.mean()-1.96*aucs.std(),aucs.mean()+1.96*aucs.std())\n",
    "\n",
    "\n",
    "    # most informative features\n",
    "    analyze('ama', vect, ama_svm)\n",
    "    print '\\n\\n\\n'\n",
    "\n",
    "    if name == 'BASELINE+ALL':\n",
    "        for featname,vals in sorted(feature_weights.items()):\n",
    "            v = np.array(vals)\n",
    "            mu = v.mean()\n",
    "            std = v.std()\n",
    "            print '%-10s:%-15s || %.2f +/- %.2f' % (featname[0],featname[1],mu,1.96*std)\n",
    "\n",
    "print strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514it [00:00, 5130.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-12 18:11:24\n",
      "BASELINE+ALL\n",
      "['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51522it [00:07, 6572.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-12 18:11:31\n",
      "num_features: 16\n",
      "\t2018-05-12 18:11:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ('age', None), 1: ('autopsy', None), 2: ('concompliant', None), 3: ('gender', 'F'), 4: ('gender', 'M'), 5: ('insurance', 'Private'), 6: ('insurance', 'Public'), 7: ('insurance', 'Self Pay'), 8: ('los', None), 9: ('race', 'Asian'), 10: ('race', 'Black'), 11: ('race', 'Hispanic'), 12: ('race', 'Native American'), 13: ('race', 'Other'), 14: ('race', 'White'), 15: ('sentiment', None)}\n",
      "patients: 39815\n",
      "Counter({0: 37359, 1: 2456})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:53<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCS:  [0.78324652 0.77849969 0.77258877 0.78107285 0.77630158 0.78631457\n",
      " 0.78667362 0.78527314 0.77617271 0.77064476 0.78380085 0.77310122\n",
      " 0.78967162 0.77809387 0.78098194 0.79000061 0.77628243 0.78179626\n",
      " 0.77897043 0.77930792 0.77893593 0.78719704 0.79322074 0.77774729\n",
      " 0.78029961 0.78037309 0.7723545  0.78490407 0.78631093 0.78732274\n",
      " 0.78235319 0.77329771 0.77591114 0.77956341 0.78691246 0.77675309\n",
      " 0.776622   0.78522874 0.78342732 0.78534675 0.76544258 0.78088814\n",
      " 0.79167358 0.79028152 0.78033415 0.7828873  0.78276036 0.77231833\n",
      " 0.77822337 0.7872044  0.79584637 0.78630402 0.7823201  0.78161895\n",
      " 0.78210182 0.78387315 0.78035303 0.77693518 0.79571572 0.78778787\n",
      " 0.78069838 0.77774305 0.78566457 0.77782712 0.78919824 0.78370198\n",
      " 0.77462228 0.78476863 0.7792839  0.78306203 0.79569874 0.76825369\n",
      " 0.78912419 0.78038647 0.78903377 0.77597883 0.78139146 0.77674471\n",
      " 0.78620727 0.78381803 0.77374165 0.78071827 0.79129715 0.78083564\n",
      " 0.78965661 0.78486894 0.78861609 0.78493896 0.79438435 0.78911179\n",
      " 0.77612298 0.77818473 0.79482327 0.78860714 0.79236023 0.800353\n",
      " 0.77944054 0.78457885 0.78401755 0.7799987 ]\n",
      "    mean:      0.7826761081383057\n",
      "    1.96*std:  0.012576353663297188\n",
      "    conf_interval: (0.7701,0.7953)\n",
      "cs\n",
      "\t('age', None)            :  0.4108\n",
      "\t('concompliant', None)   :  0.2768\n",
      "\t('race', 'White')        :  0.0978\n",
      "\t('sentiment', None)      :  0.0638\n",
      "\t('race', 'Native American'):  0.0000\n",
      "\t('race', 'Asian')        :  0.0000\n",
      "\t('insurance', 'Self Pay'):  0.0000\n",
      "\t('insurance', 'Public')  :  0.0000\n",
      "\t('gender', 'F')          :  0.0000\n",
      "\t('race', 'Other')        : -0.0849\n",
      "\t('race', 'Hispanic')     : -0.0997\n",
      "\t('race', 'Black')        : -0.1543\n",
      "\t('autopsy', None)        : -0.4202\n",
      "\t('gender', 'M')          : -0.4491\n",
      "\t('los', None)            : -0.6355\n",
      "\t('insurance', 'Private') : -1.0242\n",
      "age       :None            || 0.42 +/- 0.02\n",
      "autopsy   :None            || -0.44 +/- 0.05\n",
      "concompliant:None            || 0.27 +/- 0.04\n",
      "gender    :F               || -0.49 +/- 1.39\n",
      "gender    :M               || -0.85 +/- 1.40\n",
      "insurance :Private         || -0.94 +/- 0.29\n",
      "insurance :Public          || -0.02 +/- 0.28\n",
      "insurance :Self Pay        || -0.02 +/- 0.24\n",
      "los       :None            || -0.70 +/- 0.10\n",
      "race      :Asian           || 0.00 +/- 0.00\n",
      "race      :Black           || -0.22 +/- 0.19\n",
      "race      :Hispanic        || -0.17 +/- 0.21\n",
      "race      :Native American || 0.00 +/- 0.00\n",
      "race      :Other           || -0.12 +/- 0.17\n",
      "race      :White           || 0.06 +/- 0.15\n",
      "sentiment :None            || 0.09 +/- 0.03\n",
      "2018-05-12 18:12:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Code Status\n",
    "\n",
    "print strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "featlists = {\n",
    "                #'BASELINE'             :['age', 'los', 'insurance', 'gender'],\n",
    "                #'BASELINE+RACE'        :['age', 'los', 'insurance', 'gender', 'race'],\n",
    "                #'BASELINE+NONCOMPLIANT':['age', 'los', 'insurance', 'gender', 'noncompliant'],\n",
    "                #'BASELINE+AUTOPSY'     :['age', 'los', 'insurance', 'gender', 'autopsy'],\n",
    "                #'BASELINE+SENTIMENT'   :['age', 'los', 'insurance', 'gender', 'sentiment'],\n",
    "                'BASELINE+ALL'         :['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
    "            }\n",
    "\n",
    "cs_Y_vect = {'DNR/CMO': 1, 'Full Code': 0}\n",
    "\n",
    "feature_weights = defaultdict(list)\n",
    "\n",
    "for name,featlist in featlists.items():\n",
    "    print name\n",
    "    print featlist\n",
    "    \n",
    "    demographics_features, vect = build_features(featlist)\n",
    "    ind2feat =  { i:f for f,i in vect.vocabulary_.items() }\n",
    "    \n",
    "    print ind2feat\n",
    "\n",
    "    cs_ids = list(set(code_labels.keys()) & set(demographics_features.keys()))\n",
    "    print 'patients:', len(cs_ids)\n",
    "    \n",
    "    print Counter([cs_Y_vect[code_labels[hadm_id]] for hadm_id in cs_ids])\n",
    "\n",
    "    \n",
    "    aucs = []\n",
    "    for iteration in tqdm.tqdm(range(100)):\n",
    "\n",
    "        #print 'Iter:', iteration\n",
    "\n",
    "        # train/test split\n",
    "        cs_train_ids, cs_test_ids = data_split(cs_ids)\n",
    "\n",
    "        # select pre-computed features\n",
    "        cs_train_features = [demographics_features[hadm_id] for hadm_id in cs_train_ids]\n",
    "        cs_test_features  = [demographics_features[hadm_id] for hadm_id in cs_test_ids ]\n",
    "\n",
    "        # vectorize features\n",
    "        cs_train_X = vect.transform(cs_train_features)\n",
    "        cs_test_X  = vect.transform(cs_test_features)\n",
    "\n",
    "        # vectorize task-specific labels\n",
    "        cs_Y_vect = {'DNR/CMO': 1, 'Full Code': 0}\n",
    "        #print cs_Y_vect\n",
    "\n",
    "        # select labels\n",
    "        cs_train_Y = [cs_Y_vect[code_labels[hadm_id]] for hadm_id in cs_train_ids]\n",
    "        cs_test_Y  = [cs_Y_vect[code_labels[hadm_id]] for hadm_id in cs_test_ids ]\n",
    "\n",
    "        # fit model\n",
    "        cs_svm = LogisticRegression(C=0.1, penalty='l1', tol=0.01)\n",
    "        cs_svm.fit(cs_train_X,cs_train_Y)\n",
    "        #print cs_svm\n",
    "\n",
    "\n",
    "        # cs Model eval\n",
    "\n",
    "        # evaluate model\n",
    "        res = classification_results(cs_svm, cs_Y_vect, cs_test_X,  cs_test_Y, 'test:  cs', verbose=False)\n",
    "        aucs.append(res['auc'])\n",
    "\n",
    "        # record the weights of the features (because we average them)\n",
    "        if name == 'BASELINE+ALL':\n",
    "            for feat,val in enumerate(cs_svm.coef_.tolist()[0]):\n",
    "                featname = ind2feat[feat]\n",
    "                feature_weights[featname].append(val)\n",
    "            \n",
    "        # most informative features\n",
    "        #analyze('cs', vect, cs_svm)\n",
    "        \n",
    "    aucs = np.array(aucs)\n",
    "    print 'AUCS: ', aucs\n",
    "    print '    mean:     ', aucs.mean()\n",
    "    print '    1.96*std: ', aucs.std() * 1.96\n",
    "    print '    conf_interval: (%.4f,%.4f)' % (aucs.mean()-1.96*aucs.std(),aucs.mean()+1.96*aucs.std())\n",
    "\n",
    "\n",
    "    # most informative features\n",
    "    analyze('cs', vect, cs_svm)\n",
    "\n",
    "    if name == 'BASELINE+ALL':\n",
    "        for featname,vals in sorted(feature_weights.items()):\n",
    "            v = np.array(vals)\n",
    "            mu = v.mean()\n",
    "            std = v.std()\n",
    "            print '%-10s:%-15s || %.2f +/- %.2f' % (featname[0],featname[1],mu,1.96*std)\n",
    "    \n",
    "print strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-12 18:14:42\n",
      "BASELINE+ALL\n",
      "['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51522it [00:07, 6450.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-12 18:14:50\n",
      "num_features: 16\n",
      "\t2018-05-12 18:14:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients: 48071\n",
      "Counter({0: 42961, 1: 5110})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:54<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCS:  [0.62663672 0.62554044 0.63556882 0.62835858 0.63007437 0.63280478\n",
      " 0.64252526 0.62673491 0.62470338 0.62104103 0.63962794 0.63507344\n",
      " 0.63179274 0.63274282 0.6245192  0.63277841 0.63802319 0.63263878\n",
      " 0.63866185 0.63158812 0.64011622 0.64346836 0.62632134 0.63621778\n",
      " 0.6311151  0.63555007 0.63217218 0.63435357 0.63448743 0.63917573\n",
      " 0.63259436 0.6350698  0.63995601 0.63043239 0.63344198 0.63279208\n",
      " 0.6340524  0.62895951 0.63989063 0.62852    0.63437974 0.62799249\n",
      " 0.63266594 0.63489441 0.63226543 0.63160259 0.63506163 0.62727209\n",
      " 0.62709061 0.64039158 0.6388388  0.6375865  0.63205127 0.63197106\n",
      " 0.63126289 0.63305162 0.63643081 0.63880446 0.63810585 0.63442502\n",
      " 0.64293466 0.63015363 0.6332782  0.63691409 0.63789679 0.63672263\n",
      " 0.63472536 0.63507172 0.63622671 0.62634621 0.63057716 0.63296238\n",
      " 0.64309214 0.6351612  0.63148087 0.63210499 0.63224196 0.63462201\n",
      " 0.63555542 0.62997502 0.64211603 0.62775672 0.63095006 0.62733191\n",
      " 0.6268335  0.63452361 0.62966049 0.63259643 0.63008711 0.63862079\n",
      " 0.63925517 0.62538286 0.63429749 0.63867321 0.6311786  0.63261381\n",
      " 0.63091402 0.63120261 0.63597819 0.6346882 ]\n",
      "    mean:      0.6333897046052129\n",
      "    1.96*std:  0.00897265110740792\n",
      "    conf_interval: (0.6244,0.6424)\n",
      "mortality\n",
      "\t('age', None)            :  0.1974\n",
      "\t('sentiment', None)      :  0.1715\n",
      "\t('concompliant', None)   :  0.1493\n",
      "\t('los', None)            :  0.1029\n",
      "\t('autopsy', None)        :  0.0404\n",
      "\t('race', 'Other')        :  0.0170\n",
      "\t('race', 'Native American'):  0.0000\n",
      "\t('gender', 'F')          : -0.1644\n",
      "\t('race', 'Asian')        : -0.1955\n",
      "\t('gender', 'M')          : -0.2315\n",
      "\t('race', 'White')        : -0.3306\n",
      "\t('race', 'Black')        : -0.5819\n",
      "\t('race', 'Hispanic')     : -0.6830\n",
      "\t('insurance', 'Self Pay'): -0.8813\n",
      "\t('insurance', 'Public')  : -1.0708\n",
      "\t('insurance', 'Private') : -1.4827\n",
      "age       :None            || 0.20 +/- 0.02\n",
      "autopsy   :None            || 0.02 +/- 0.02\n",
      "concompliant:None            || 0.16 +/- 0.03\n",
      "gender    :F               || -0.59 +/- 0.99\n",
      "gender    :M               || -0.67 +/- 0.99\n",
      "insurance :Private         || -0.89 +/- 0.95\n",
      "insurance :Public          || -0.50 +/- 0.95\n",
      "insurance :Self Pay        || -0.21 +/- 0.68\n",
      "los       :None            || 0.08 +/- 0.03\n",
      "race      :Asian           || -0.05 +/- 0.15\n",
      "race      :Black           || -0.53 +/- 0.31\n",
      "race      :Hispanic        || -0.58 +/- 0.34\n",
      "race      :Native American || 0.00 +/- 0.00\n",
      "race      :Other           || 0.15 +/- 0.30\n",
      "race      :White           || -0.26 +/- 0.30\n",
      "sentiment :None            || 0.16 +/- 0.03\n",
      "\n",
      "\n",
      "\n",
      "2018-05-12 18:15:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Mortality\n",
    "\n",
    "print strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "featlists = {\n",
    "                #'BASELINE'             :['age', 'los', 'insurance', 'gender'],\n",
    "                #'BASELINE+RACE'        :['age', 'los', 'insurance', 'gender', 'race'],\n",
    "                #'BASELINE+NONCOMPLIANT':['age', 'los', 'insurance', 'gender', 'noncompliant'],\n",
    "                #'BASELINE+AUTOPSY'     :['age', 'los', 'insurance', 'gender', 'autopsy'],\n",
    "                #'BASELINE+SENTIMENT'   :['age', 'los', 'insurance', 'gender', 'sentiment'],\n",
    "                'BASELINE+ALL'         :['age', 'los', 'insurance', 'gender', 'race', 'noncompliant', 'autopsy', 'sentiment']\n",
    "            }\n",
    "mortality_Y_vect = {'deceased': 1, 'survived': 0}\n",
    "\n",
    "feature_weights = defaultdict(list)\n",
    "\n",
    "\n",
    "for name,featlist in featlists.items():\n",
    "    print name\n",
    "    print featlist\n",
    "    \n",
    "    demographics_features, vect = build_features(featlist)\n",
    "    ind2feat =  { i:f for f,i in vect.vocabulary_.items() }\n",
    "\n",
    "    mortality_ids = list(set(mortality_labels.keys()) & set(demographics_features.keys()))\n",
    "    print 'patients:', len(mortality_ids)\n",
    "    \n",
    "    print Counter([mortality_Y_vect[mortality_labels[hadm_id]] for hadm_id in mortality_ids])\n",
    "\n",
    "    aucs = []\n",
    "    for iteration in tqdm.tqdm(range(100)):\n",
    "\n",
    "        #print 'Iter:', iteration\n",
    "\n",
    "\n",
    "        # train/test split\n",
    "        mortality_train_ids, mortality_test_ids = data_split(mortality_ids)\n",
    "\n",
    "        # select pre-computed features\n",
    "        mortality_train_features = [demographics_features[hadm_id] for hadm_id in mortality_train_ids]\n",
    "        mortality_test_features  = [demographics_features[hadm_id] for hadm_id in mortality_test_ids ]\n",
    "\n",
    "        # vectorize features\n",
    "        mortality_train_X = vect.transform(mortality_train_features)\n",
    "        mortality_test_X  = vect.transform(mortality_test_features)\n",
    "\n",
    "        # vectorize task-specific labels\n",
    "        #print mortality_Y_vect\n",
    "\n",
    "        # select labels\n",
    "        mortality_train_Y = [mortality_Y_vect[mortality_labels[hadm_id]] for hadm_id in mortality_train_ids]\n",
    "        mortality_test_Y  = [mortality_Y_vect[mortality_labels[hadm_id]] for hadm_id in mortality_test_ids ]\n",
    "\n",
    "        # fit model\n",
    "        mortality_svm = LogisticRegression(C=0.1, penalty='l1', tol=0.01)\n",
    "        mortality_svm.fit(mortality_train_X,mortality_train_Y)\n",
    "        #print mortality_svm\n",
    "\n",
    "\n",
    "        # mortality Model eval\n",
    "\n",
    "        # evaluate model\n",
    "        res = classification_results(mortality_svm, mortality_Y_vect, mortality_test_X, mortality_test_Y, 'test:  mortality', verbose=False)\n",
    "        aucs.append(res['auc'])\n",
    "\n",
    "        # record the weights of the features (because we average them)\n",
    "        if name == 'BASELINE+ALL':\n",
    "            for feat,val in enumerate(mortality_svm.coef_.tolist()[0]):\n",
    "                featname = ind2feat[feat]\n",
    "                feature_weights[featname].append(val)\n",
    "            \n",
    "        # most informative features\n",
    "        #analyze('mortality', vect, mortality_svm)\n",
    "        \n",
    "    aucs = np.array(aucs)\n",
    "    print 'AUCS: ', aucs\n",
    "    print '    mean:     ', aucs.mean()\n",
    "    print '    1.96*std: ', aucs.std() * 1.96\n",
    "    print '    conf_interval: (%.4f,%.4f)' % (aucs.mean()-1.96*aucs.std(),aucs.mean()+1.96*aucs.std())\n",
    "\n",
    "\n",
    "    # most informative features\n",
    "    analyze('mortality', vect, mortality_svm)\n",
    "\n",
    "    # foo\n",
    "\n",
    "\n",
    "    if name == 'BASELINE+ALL':\n",
    "        for featname,vals in sorted(feature_weights.items()):\n",
    "            v = np.array(vals)\n",
    "            mu = v.mean()\n",
    "            std = v.std()\n",
    "            print '%-10s:%-15s || %.2f +/- %.2f' % (featname[0],featname[1],mu,1.96*std)\n",
    "            \n",
    "    print '\\n\\n'\n",
    "    \n",
    "print strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autopsy\n",
      "\tmost  trust 0.12787967718268525\n",
      "\tleast trust 0.08814726129901228\n",
      "noncompliant\n",
      "\tmost  trust 0.043835616438356165\n",
      "\tleast trust 0.13696418085731063\n",
      "sentiment\n",
      "\tmost  trust 0.07846549009859972\n",
      "\tleast trust 0.14477730948855222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = {'noncompliant':noncompliant_dict, 'autopsy':autopsy_dict, 'sentiment':sentiment_dict}\n",
    "\n",
    "for metric,scores in metrics.items():\n",
    "    print metric\n",
    "\n",
    "    vals = sorted(scores.values())\n",
    "    n = len(vals)\n",
    "    t1 = vals[1*n/4]\n",
    "    t2 = vals[2*n/4]\n",
    "    t3 = vals[3*n/4]\n",
    "\n",
    "    lowest  = [hadm_id for hadm_id,score in scores.items() if     score<=t1]\n",
    "    highest = [hadm_id for hadm_id,score in scores.items() if t3< score    ]\n",
    "\n",
    "    def mort_rate(label, hadm_ids):\n",
    "        cohort = mortality.loc[mortality['hadm_id'].isin(hadm_ids)]\n",
    "        print '\\t', label, sum(cohort['hospital_expire_flag'].values)/float(len(cohort))\n",
    "\n",
    "    mort_rate('most  trust', lowest)\n",
    "    mort_rate('least trust', highest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
